# =============================================================================
#  CONFIGURATION DE LA STACK IA
# =============================================================================
#
#  Ce fichier contient TOUTES les variables que vous pouvez modifier
#  pour personnaliser votre installation.
#
#  Comment l'utiliser :
#    1. Copiez ce fichier : cp .env.example .env
#    2. Modifiez les valeurs selon vos besoins
#    3. Lancez la stack : docker compose up -d
#
# =============================================================================


# =============================================================================
#  MODÈLE LLM (le cerveau de l'IA)
# =============================================================================
#
#  Choisissez UN modèle parmi ceux ci-dessous.
#  Décommentez la ligne du modèle souhaité et commentez les autres.
#
#  IMPORTANT : Un seul modèle tourne à la fois. Pour en changer :
#    1. Modifiez cette variable
#    2. Relancez : docker compose up -d vllm --force-recreate
#
#  Le modèle sera téléchargé automatiquement au premier lancement.
#  Cela peut prendre plusieurs minutes selon votre connexion internet.
#
# --- Modèles recommandés (du plus léger au plus lourd) ---
#
#  Gemma 3 4B  → ~3 Go VRAM  → Rapide, bon pour les tests
VLLM_MODEL=google/gemma-3-4b-it
#
#  Llama 3 8B  → ~6-8 Go VRAM → Bon équilibre performance/ressources
#  (Nécessite un token HuggingFace, voir HF_TOKEN ci-dessous)
#VLLM_MODEL=meta-llama/Meta-Llama-3.1-8B-Instruct
#
#  GLM-4 9B    → ~6-8 Go VRAM → Bon en multilingue
#VLLM_MODEL=THUDM/glm-4-9b-chat
#
#  DeepSeek R1 14B Distill → ~10-12 Go VRAM → Raisonnement avancé
#VLLM_MODEL=deepseek-ai/DeepSeek-R1-Distill-Qwen-14B
#
#  Gemma 3 27B → ~18-20 Go VRAM → Le plus puissant, nécessite beaucoup de VRAM
#VLLM_MODEL=google/gemma-3-27b-it
#

# Taille maximale du contexte (en tokens)
# Plus c'est grand, plus le modèle peut "lire" de texte, mais plus ça consomme de VRAM
# Valeurs recommandées : 2048, 4096, 8192
VLLM_MAX_MODEL_LEN=4096

# Pourcentage de mémoire GPU à utiliser (entre 0.5 et 0.95)
# Si vous avez des erreurs "Out of Memory", baissez cette valeur
# 0.85 = utilise 85% de la VRAM disponible
VLLM_GPU_MEM=0.85


# =============================================================================
#  MODÈLE D'EMBEDDING (transforme le texte en vecteurs pour la recherche)
# =============================================================================
#
#  BGE-M3 est le modèle recommandé :
#    - Supporte plus de 100 langues (dont le français)
#    - Peut traiter des textes jusqu'à 8192 tokens
#    - Consomme ~2 Go de VRAM
#
#  Normalement, vous n'avez pas besoin de changer cette valeur.
#
EMBEDDING_MODEL=BAAI/bge-m3


# =============================================================================
#  TOKEN HUGGINGFACE (pour les modèles protégés)
# =============================================================================
#
#  Certains modèles (Llama, Gemma, etc.) nécessitent d'accepter une licence
#  sur HuggingFace avant de pouvoir les télécharger.
#
#  Pour obtenir un token :
#    1. Créez un compte sur https://huggingface.co
#    2. Allez sur https://huggingface.co/settings/tokens
#    3. Créez un token avec les droits "Read"
#    4. Collez-le ici
#
#  Si vous utilisez un modèle qui ne nécessite pas de token, laissez vide.
#
HF_TOKEN=


# =============================================================================
#  CLÉS API (sécurité interne entre les services)
# =============================================================================
#
#  Ces clés sécurisent la communication entre les services Docker.
#  Changez-les si votre serveur est accessible depuis l'extérieur.
#
#  Pour générer une clé sécurisée :
#    openssl rand -hex 32
#
VLLM_API_KEY=sk-local-key
QDRANT_API_KEY=qdrant-local-key


# =============================================================================
#  PORTS (sur quels ports les services sont accessibles)
# =============================================================================
#
#  Modifiez uniquement si ces ports sont déjà utilisés sur votre serveur.
#
#  Open WebUI  : l'interface dans le navigateur
WEBUI_PORT=3000
#  vLLM        : l'API du LLM
VLLM_PORT=8000
#  TEI         : l'API d'embedding
TEI_PORT=8081
#  Qdrant      : l'API de la base vectorielle
QDRANT_PORT=6333
#  Docling     : l'API d'extraction de documents
DOCLING_PORT=5001


# =============================================================================
#  NOM DE L'INTERFACE
# =============================================================================
#
#  Le nom affiché en haut de l'interface Open WebUI.
#
WEBUI_NAME=Mon Assistant IA
