# =============================================================================
#  CONFIGURATION DE LA STACK IA (Open Source)
# =============================================================================
#
#  Ce fichier contient toutes les variables que vous pouvez modifier
#  pour personnaliser votre installation.
#
#  Utilisation :
#    1. Copiez ce fichier : cp .env.example .env
#    2. Modifiez les valeurs selon vos besoins
#    3. Lancez la stack : docker compose up -d
#
# =============================================================================

# =============================================================================
#  MODÈLE LLM (le cerveau de l'IA)
# =============================================================================
#
#  Choisissez UN modèle parmi ceux ci-dessous.
#  Décommentez la ligne du modèle souhaité et commentez les autres.
#
#  IMPORTANT : Un seul modèle tourne à la fois. Pour en changer :
#    1. Modifiez cette variable
#    2. Relancez : docker compose up -d vllm --force-recreate
#
#  Le modèle sera téléchargé automatiquement au premier lancement.
#  Cela peut prendre plusieurs minutes selon votre connexion internet.
#
# --- Modèles disponibles (du plus léger au plus lourd) ---
#
#  1) Mistral 7B        → ~6-7 Go VRAM → Très rapide
VLLM_MODEL=mistralai/Mistral-7B-Instruct-v0.2
#
#  2) Llama 2 7B        → ~6-8 Go VRAM → Bon équilibre performance/ressources
#VLLM_MODEL=meta-llama/Llama-2-7b-chat-hf
#
#  3) Llama 2 13B       → ~12-14 Go VRAM → Plus puissant
#VLLM_MODEL=meta-llama/Llama-2-13b-chat-hf
#
#  4) RedPajama 7B      → ~7 Go VRAM → Instruction fine-tuning
#VLLM_MODEL=RedPajama-INCITE/RedPajama-INCITE-7B-Instruct
#
#  5) RedPajama 13B     → ~12-13 Go VRAM → Plus puissant
#VLLM_MODEL=RedPajama-INCITE-13B-Instruct
#

# Taille maximale du contexte (en tokens)
VLLM_MAX_MODEL_LEN=4096

# Pourcentage de mémoire GPU à utiliser (entre 0.5 et 0.95)
VLLM_GPU_MEM=0.85

# =============================================================================
#  MODÈLE D'EMBEDDING (transforme le texte en vecteurs pour la recherche)
# =============================================================================
EMBEDDING_MODEL=BAAI/bge-m3

# =============================================================================
#  TOKEN HUGGINGFACE (pour les modèles protégés)
# =============================================================================
HF_TOKEN=

# =============================================================================
#  CLÉS API (sécurité interne entre les services)
# =============================================================================
VLLM_API_KEY=sk-local-key
QDRANT_API_KEY=qdrant-local-key

# =============================================================================
#  PORTS (sur quels ports les services sont accessibles)
# =============================================================================
WEBUI_PORT=3000
VLLM_PORT=8000
TEI_PORT=8081
QDRANT_PORT=6333
DOCLING_PORT=5001

# =============================================================================
#  NOM DE L'INTERFACE
# =============================================================================
WEBUI_NAME=Mon Assistant IA
